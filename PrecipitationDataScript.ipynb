{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for Downloading ERA5 Global Precipitation Average Data \n",
    "<b>Writes to json file rainfall_avgs.json</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize notebook environment.\n",
    "%matplotlib inline\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import botocore\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import xarray as xr\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_bucket = 'era5-pds'\n",
    "\n",
    "# AWS access / secret keys required\n",
    "# s3 = boto3.resource('s3')\n",
    "# bucket = s3.Bucket(era5_bucket)\n",
    "\n",
    "# No AWS keys required\n",
    "client = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_presigned_url(bucket_name, object_name, expiration=3600):\n",
    "    \"\"\"Generate a presigned URL to share an S3 object\n",
    "\n",
    "    :param bucket_name: string\n",
    "    :param object_name: string\n",
    "    :param expiration: Time in seconds for the presigned URL to remain valid\n",
    "    :return: Presigned URL as string. If error, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a presigned URL for the S3 object\n",
    "    s3_client = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))\n",
    "    try:\n",
    "        response = s3_client.generate_presigned_url('get_object',\n",
    "                                                    Params={'Bucket': bucket_name,\n",
    "                                                            'Key': object_name},\n",
    "                                                    ExpiresIn=expiration)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return None\n",
    "\n",
    "    # The response contains the presigned URL\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = create_presigned_url(era5_bucket, '1979/01/main.nc')\n",
    "# print(url);\n",
    "# response = requests.get(url)\n",
    "# print(response)\n",
    "# ds = xr.open_dataset(url)\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions for ERA5 Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Direct file downloads to a separate folder to avoid clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment ds_meta.info() to have main.nc file contents printed\n",
    "def downloadERA5Data(prefix, file_type, year, month):\n",
    "    metadata_file = file_type \n",
    "    metadata_key = prefix + file_type\n",
    "    #print(metadata_file)\n",
    "    #print(metadata_key)\n",
    "    client.download_file(era5_bucket, metadata_key, metadata_file)\n",
    "    ds_meta = xr.open_dataset(file_type, decode_times=False)\n",
    "    #ds_meta.info() \n",
    "    \n",
    "    # select date and variable of interest\n",
    "    date = datetime.date(year,month,1)\n",
    "    #print(\"Getting main.nc file for \", date)\n",
    "    var = 'precipitation_amount_1hour_Accumulation'\n",
    "\n",
    "    # file path patterns for remote S3 objects and corresponding local file\n",
    "    s3_data_ptrn = '{year}/{month}/data/{var}.nc'\n",
    "    data_file_ptrn = '{year}{month}_{var}.nc'\n",
    "\n",
    "    year = date.strftime('%Y')\n",
    "    month = date.strftime('%m')\n",
    "    s3_data_key = s3_data_ptrn.format(year=year, month=month, var=var)\n",
    "    data_file = data_file_ptrn.format(year=year, month=month, var=var)\n",
    "\n",
    "    if not os.path.isfile('DataFiles/'+ data_file): # check if file already exists\n",
    "        print(\"Downloading %s from S3...\" % s3_data_key)\n",
    "        client.download_file(era5_bucket, s3_data_key, data_file)\n",
    "        return data_file\n",
    "    else:\n",
    "#         print(\"File already exists on machine\")\n",
    "        return data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at top level prefixes of S3 Data Bucket for ERA5 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = client.get_paginator('list_objects')\n",
    "result = paginator.paginate(Bucket=era5_bucket, Delimiter='/')\n",
    "data_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in result.search('CommonPrefixes'):\n",
    "    keys = []\n",
    "    year = prefix.get('Prefix').split('/')[0]\n",
    "    #print(year)\n",
    "    response = client.list_objects_v2(Bucket=era5_bucket, Prefix=year)\n",
    "    response_meta = response.get('ResponseMetadata')\n",
    "    \n",
    "    if response_meta.get('HTTPStatusCode') == 200:\n",
    "        contents = response.get('Contents')\n",
    "        if contents == None:\n",
    "            print(\"No objects are available for %s\" % date.strftime('%B, %Y'))\n",
    "        else:\n",
    "            for obj in contents:\n",
    "                keys.append(obj.get('Key'))\n",
    "            # Getting the first month available for the current year \n",
    "            month = keys[0].split('/', 2)[1];\n",
    "            for k in keys:\n",
    "                path_split = k.split('/', 3)\n",
    "                current_month = path_split[1]\n",
    "\n",
    "                if(len(path_split) >= 4):\n",
    "                    file_type = path_split[3] # index of file type/extension\n",
    "                    if(file_type == 'precipitation_amount_1hour_Accumulation.nc'):\n",
    "                        # There is precipitation accumulation data set for the current month ==> download this months dataset to query data\n",
    "                        #print(k)\n",
    "                        prefix = year + \"/\" + month + \"/\"\n",
    "                        data_file = downloadERA5Data(prefix, 'main.nc', int(year), int(month.lstrip(\"0\"))) # Having errors querying precipitation data directly\n",
    "                        data_files.append(data_file);\n",
    "                if(current_month != month):\n",
    "                    month = current_month\n",
    "                    #print(month)\n",
    "#             break; # currently breaking query after 1 year \n",
    "    else:\n",
    "        print(\"There was an error with your request.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a list of locations to gather data from for varying El Nino affects \n",
    "<b> Access locations with locs_(lr, d, or fr) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locations with lower than average detected rainfall\n",
      "{'name': 'el_salvador', 'lon': 13.7942, 'lat': 88.8965}\n",
      "{'name': 'honduras', 'lon': 15.2, 'lat': 86.2419}\n",
      "{'name': 'nicaragua', 'lon': 12.8654, 'lat': 85.2072}\n",
      "{'name': 'haiti', 'lon': 18.9712, 'lat': 72.2852}\n",
      "{'name': 'se_brazil', 'lon': 20.3332, 'lat': 46.2092}\n",
      "---------------------------------------------------\n",
      "Locations with drought level rainfall\n",
      "{'name': 'ethiopia', 'lon': 9.145, 'lat': 40.4897}\n",
      "{'name': 'kenya', 'lon': 0.0236, 'lat': 37.9062}\n",
      "{'name': 'somalia', 'lon': 5.1521, 'lat': 46.1996}\n",
      "{'name': 'malawi', 'lon': 13.2543, 'lat': 34.3015}\n",
      "---------------------------------------------------\n",
      "Locations with higher than average detected rainfall\n",
      "{'name': 'argentina', 'lon': 38.4161, 'lat': 63.6167}\n",
      "{'name': 'guatemala', 'lon': 15.7835, 'lat': 90.2308}\n",
      "{'name': 'peru', 'lon': 9.19, 'lat': 75.0152}\n",
      "{'name': 'botswana', 'lon': 22.3285, 'lat': 24.6849}\n",
      "{'name': 'zimbabwe', 'lon': 19.0154, 'lat': 29.1549}\n",
      "{'name': 'mozambique', 'lon': 18.6657, 'lat': 35.5296}\n",
      "{'name': 'south_africa', 'lon': 30.5595, 'lat': 22.9375}\n"
     ]
    }
   ],
   "source": [
    "# locations w/ lower than average rainfall\n",
    "locs_lr = [\n",
    "    {'name': 'el_salvador', 'lon': 13.7942, 'lat': 88.8965},\n",
    "    {'name': 'honduras', 'lon': 15.2000, 'lat': 86.2419},\n",
    "    {'name': 'nicaragua', 'lon': 12.8654, 'lat': 85.2072},\n",
    "    {'name': 'haiti', 'lon': 18.9712, 'lat': 72.2852},\n",
    "    {'name': 'se_brazil', 'lon': 20.3332, 'lat': 46.2092},\n",
    "]\n",
    "\n",
    "# locations w/ drought risk level rainfall\n",
    "locs_d = [\n",
    "    {'name': 'ethiopia', 'lon': 9.1450, 'lat': 40.4897},\n",
    "    {'name': 'kenya', 'lon': 0.0236, 'lat': 37.9062},\n",
    "    {'name': 'somalia', 'lon': 5.1521, 'lat': 46.1996},\n",
    "    {'name': 'malawi', 'lon': 13.2543, 'lat': 34.3015},\n",
    "    \n",
    "]\n",
    "\n",
    "# locations w/ flood risk \n",
    "locs_fr = [\n",
    "    {'name': 'argentina', 'lon': 38.4161, 'lat': 63.6167},\n",
    "    {'name': 'guatemala', 'lon': 15.7835, 'lat': 90.2308},\n",
    "    {'name': 'peru', 'lon': 9.1900, 'lat': 75.0152},\n",
    "    {'name': 'botswana', 'lon': 22.3285, 'lat': 24.6849},\n",
    "    {'name': 'zimbabwe', 'lon': 19.0154 , 'lat': 29.1549},\n",
    "    {'name': 'mozambique', 'lon': 18.6657, 'lat': 35.5296},\n",
    "    {'name': 'south_africa', 'lon': 30.5595, 'lat': 22.9375},\n",
    "]\n",
    "\n",
    "\n",
    "# convert westward longitudes to degrees east\n",
    "print(\"Locations with lower than average detected rainfall\")\n",
    "for l in locs_lr:\n",
    "    print(l)\n",
    "    if l['lon'] < 0:\n",
    "        l['lon'] = 360 + l['lon']\n",
    "  \n",
    "print('---------------------------------------------------')\n",
    "print(\"Locations with drought level rainfall\")\n",
    "for l in locs_d:\n",
    "    print(l)\n",
    "    if l['lon'] < 0:\n",
    "        l['lon'] = 360 + l['lon']\n",
    "\n",
    "print('---------------------------------------------------')\n",
    "print(\"Locations with higher than average detected rainfall\")\n",
    "for l in locs_fr:\n",
    "    print(l)\n",
    "    if l['lon'] < 0:\n",
    "        l['lon'] = 360 + l['lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for getting data on each location from a provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrecipitationData(ds):\n",
    "    var = 'precipitation_amount_1hour_Accumulation' # Var to be changed to name of corresponding country \n",
    "    ds_locs_fr = xr.Dataset()\n",
    "    ds_locs_lr = xr.Dataset()\n",
    "    ds_locs_d = xr.Dataset()\n",
    "    \n",
    "    # interate through the locations and create a dataset\n",
    "    # containing the temperature values for each location\n",
    "    for l in locs_fr:\n",
    "        name = l['name']\n",
    "        lon = l['lon']\n",
    "        lat = l['lat']\n",
    "        var_name = name\n",
    "\n",
    "        ds2 = ds.sel(lon=lon, lat=lat, method='nearest')\n",
    "\n",
    "        lon_attr = '%s_lon' % name\n",
    "        lat_attr = '%s_lat' % name\n",
    "\n",
    "        ds2.attrs[lon_attr] = ds2.lon.values.tolist()\n",
    "        ds2.attrs[lat_attr] = ds2.lat.values.tolist()\n",
    "        ds2 = ds2.rename({var : var_name}).drop(('lat', 'lon'))\n",
    "\n",
    "        ds_locs_fr = xr.merge([ds_locs_fr, ds2])\n",
    "\n",
    "    ds_locs_fr.data_vars\n",
    "\n",
    "    for l in locs_d:\n",
    "        name = l['name']\n",
    "        lon = l['lon']\n",
    "        lat = l['lat']\n",
    "        var_name = name\n",
    "\n",
    "        ds2 = ds.sel(lon=lon, lat=lat, method='nearest')\n",
    "\n",
    "        lon_attr = '%s_lon' % name\n",
    "        lat_attr = '%s_lat' % name\n",
    "\n",
    "        ds2.attrs[lon_attr] = ds2.lon.values.tolist()\n",
    "        ds2.attrs[lat_attr] = ds2.lat.values.tolist()\n",
    "        ds2 = ds2.rename({var : var_name}).drop(('lat', 'lon'))\n",
    "\n",
    "        ds_locs_d = xr.merge([ds_locs_d, ds2])\n",
    "\n",
    "    ds_locs_d.data_vars\n",
    "\n",
    "    for l in locs_lr:\n",
    "        name = l['name']\n",
    "        lon = l['lon']\n",
    "        lat = l['lat']\n",
    "        var_name = name\n",
    "\n",
    "        ds2 = ds.sel(lon=lon, lat=lat, method='nearest')\n",
    "\n",
    "        lon_attr = '%s_lon' % name\n",
    "        lat_attr = '%s_lat' % name\n",
    "\n",
    "        ds2.attrs[lon_attr] = ds2.lon.values.tolist()\n",
    "        ds2.attrs[lat_attr] = ds2.lat.values.tolist()\n",
    "        ds2 = ds2.rename({var : var_name}).drop(('lat', 'lon'))\n",
    "\n",
    "        ds_locs_lr = xr.merge([ds_locs_lr, ds2])\n",
    "\n",
    "    ds_locs_lr.data_vars\n",
    "\n",
    "#     print(ds_locs_fr.data_vars)\n",
    "#     print(ds_locs_lr.data_vars)\n",
    "#     print(ds_locs_d.data_vars)\n",
    "    return [ds_locs_fr, ds_locs_lr, ds_locs_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = []\n",
    "for file in data_files:\n",
    "    ds = xr.open_dataset('DataFiles/'+file)\n",
    "    data_sets.append(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing monthly rainfall averages for at risk areas in 'rainfall_avgs' dictionary\n",
    "<b> Structure outlined below (DO NOT RUN UNLESS AN UPDATE TO JSON IS NECESSARY) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# rainfally_avgs = {\n",
    "#     1979: {\n",
    "#         01: {\n",
    "#             higher_avg: {\n",
    "#                 loc1: mean rainfall\n",
    "#                 loc2: ''\n",
    "#                 ...\n",
    "#                 locn: '' \n",
    "#             }, \n",
    "#             lower_avg: {...},\n",
    "#             drought: {...}\n",
    "#         }\n",
    "#         ..\n",
    "#         12: {...}\n",
    "#     }\n",
    "#     ...\n",
    "#     2018: {...}\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rainfall_avgs = {}\n",
    "# # data_entry = {}\n",
    "# for ds in data_sets: # Loop through array indexes with data for each month\n",
    "#     data_entry = {}\n",
    "#     precip_ds = getPrecipitationData(ds)\n",
    "#     i = 0\n",
    "#     for m in precip_ds: # Loop through locs_fr/lr/d for current month\n",
    "#         year = ''\n",
    "#         month = ''\n",
    "#         if(i == 0):\n",
    "#             risk = 'higher_avg'\n",
    "#         elif(i == 1):\n",
    "#             risk = 'lower_avg'\n",
    "#         elif(i == 2):\n",
    "#             risk ='drought_risk'\n",
    "#         else:\n",
    "#             break\n",
    "#         i+=1 \n",
    "        \n",
    "#         for val in m.coords: # Loop through coordinate values to get associated time \n",
    "#             data_entry[risk] = {}\n",
    "#             #print(data_entry)\n",
    "#             m_mean = m.mean()\n",
    "#             date = m[val].values[0]\n",
    "#             year = pd.to_datetime(date).year\n",
    "#             month = pd.to_datetime(date).month\n",
    "\n",
    "#             if not year in rainfall_avgs:\n",
    "#                 rainfall_avgs[year] = {month: {}}\n",
    "            \n",
    "#             for loc in m_mean.data_vars:\n",
    "#                 location = loc\n",
    "#                 avg_rainfall = m_mean[loc].values * 24 * 1488\n",
    "#                 data_entry[risk][location] = avg_rainfall\n",
    "#                 rainfall_avgs[year][month] = data_entry\n",
    "# #                 print(year, month)\n",
    "# #                 print(location , avg_rainfall, 'm')\n",
    "# #                 print(rainfall_avgs[year][month])\n",
    "# #                 print(\"***********************************************************\")\n",
    "# #                 if(month > 1):\n",
    "# #                      print(rainfall_avgs[year][month-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oni = pd.read_csv('https://query.data.world/s/72sjhekyakcsiehjtd7yjuhccedtae')\n",
    "oni\n",
    "print(json.dumps(rainfall_avgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rainfall_avgs.json', 'w') as fp:\n",
    "    json.dump(rainfall_avgs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
